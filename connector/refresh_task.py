#!/usr/bin/env python3
"""
Inflectiv Living Dataset â€” Refresh Task
========================================
This script is called by the scheduler to refresh a living dataset.
It researches the topic, diffs against the previous version, structures
the new data, updates the registry, and publishes to Inflectiv.

This is the engine behind Living Datasets â€” run automatically on schedule.

Usage:
    python refresh_task.py --id ld_001
    python refresh_task.py --id ld_001 --demo
    python refresh_task.py --all          # refresh all due datasets

Scheduler usage (via Agent Zero scheduler):
    Called automatically by the task scheduler with the living dataset ID.
"""

import argparse
import json
import os
import subprocess
import sys
from datetime import datetime, timedelta
from pathlib import Path

BASE_DIR       = Path(__file__).parent
INTERESTS_FILE = BASE_DIR / "interests.json"
REGISTRY_FILE  = BASE_DIR / "registry.json"
PROJECT_DIR    = BASE_DIR.parent
SCRIPTS_DIR    = PROJECT_DIR / "skills" / "inflectiv" / "scripts"

SCHEDULE_MAP = {
    "hourly": timedelta(hours=1),
    "daily":  timedelta(days=1),
    "weekly": timedelta(weeks=1),
}

# â”€â”€ Vault Integration â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def vault_contribute(content: str, title: str, auto_approve: bool = False) -> bool:
    """Contribute refreshed dataset summary to local vault if configured."""
    vault_url   = os.environ.get("VAULT_URL", "").rstrip("/")
    vault_token = os.environ.get("VAULT_TOKEN", "")
    if not vault_url or not vault_token:
        return False
    try:
        import urllib.request, json as _json
        payload = _json.dumps({"content": content}).encode()
        req = urllib.request.Request(
            f"{vault_url}/vault/contribute",
            data=payload,
            headers={"Content-Type": "application/json", "X-Vault-Token": vault_token},
            method="POST"
        )
        with urllib.request.urlopen(req, timeout=10) as resp:
            data = _json.loads(resp.read())
        cid      = data.get("contribution_id", "")
        category = (data.get("top_category") or {}).get("category", "unknown")
        print(f"  ðŸ¦ Vault: staged [{category}] id={cid[:8]}...")
        if auto_approve and cid:
            req2 = urllib.request.Request(
                f"{vault_url}/vault/pending/{cid}/approve",
                data=b"",
                headers={"X-Vault-Token": vault_token},
                method="POST"
            )
            with urllib.request.urlopen(req2, timeout=10) as r2:
                ack = _json.loads(r2.read())
            print(f"  ðŸ¦ Vault: auto-approved â†’ {ack.get('message','ok')}")
        return True
    except Exception as e:
        print(f"  âš ï¸  Vault contribute skipped: {e}")
        return False



def load_json(path):
    if not Path(path).exists():
        return {}
    with open(path) as f:
        return json.load(f)

def save_json(path, data):
    with open(path, "w") as f:
        json.dump(data, f, indent=2)


def refresh_dataset(ld_id: str, demo: bool = False, api_key: str = None):
    """Refresh a living dataset by ID."""

    registry  = load_json(REGISTRY_FILE)
    interests = load_json(INTERESTS_FILE)

    datasets = registry.get("living_datasets", [])
    topics   = {i["id"]: i for i in interests.get("interests", [])}

    target = next((d for d in datasets if d["id"] == ld_id), None)
    if not target:
        print(f"[ERROR] No living dataset found with ID: {ld_id}", file=sys.stderr)
        sys.exit(1)

    topic = topics.get(target["topic_id"], {})
    topic_text = topic.get("topic", target["title"])
    schedule   = target.get("refresh_schedule", "daily")

    print(f"\n{'â•'*60}")
    print(f"  ðŸ”„  INFLECTIV LIVING DATASET REFRESH")
    print(f"  Dataset: {target['title']}")
    print(f"  Topic:   {topic_text}")
    print(f"  Version: v{target['version']} â†’ v{target['version']+1}")
    print(f"  Mode:    {'DEMO' if demo else 'LIVE'}")
    print(f"{'â•'*60}")

    now = datetime.utcnow()
    next_run = (now + SCHEDULE_MAP.get(schedule, timedelta(days=1))).isoformat() + "Z"

    # â”€â”€ Step 1: Re-query Inflectiv to check if dataset was already updated â”€â”€
    print("\n[1/4] ðŸ” Query-Before-Refresh: checking Inflectiv for newer data...")
    query_script = SCRIPTS_DIR / "query_datasets.py"
    query_cmd = [sys.executable, str(query_script), "--query", topic_text, "--demo"]
    result = subprocess.run(query_cmd, capture_output=True, text=True)
    print(result.stdout[:300] if result.stdout else "  (no output)")

    # â”€â”€ Step 2: Research update (in production this calls search + document_query) â”€â”€
    print("\n[2/4] ðŸŒ Researching latest updates on topic...")
    # In production: would call search_engine + document_query + diff against previous version
    # For demo: simulate discovering new data
    if demo:
        new_findings = {
            "refresh_note": f"Refresh v{target['version']+1} â€” automated update",
            "new_adopters_found": 2,
            "changelog_summary": f"Added 2 new adopters, updated V2 status, refreshed adoption metrics",
            "timestamp": now.isoformat() + "Z"
        }
        print(f"  âœ… Found updates: {new_findings['changelog_summary']}")
    else:
        print("  â„¹ï¸  In production: executes search_engine + document_query pipeline")
        print("  â„¹ï¸  Diffs against previous version, only publishes on meaningful changes")
        new_findings = {"changelog_summary": "Live refresh executed"}

    # â”€â”€ Step 3: Update dataset file â”€â”€
    print("\n[3/4] ðŸ“ Updating dataset...")
    file_path = target.get("file_path")
    if file_path and Path(file_path).exists():
        existing = load_json(Path(file_path))
        existing["meta"]["version"] = f"1.{target['version']+1}.0"
        existing["meta"]["last_refreshed"] = now.isoformat() + "Z"
        existing["meta"]["refresh_count"] = target["refresh_count"] + 1
        if demo:
            existing["meta"]["last_refresh_summary"] = new_findings["changelog_summary"]
        save_json(Path(file_path), existing)
        print(f"  âœ… Updated: {file_path}")
    else:
        print("  â„¹ï¸  No existing file â€” would create new dataset on first run")

    # â”€â”€ Step 4: Publish updated dataset â”€â”€
    print("\n[4/4] ðŸ“¤ Publishing updated dataset to Inflectiv...")
    pub_script = SCRIPTS_DIR / "publish_dataset.py"
    if file_path and Path(file_path).exists():
        pub_args = [
            sys.executable, str(pub_script),
            "--title", target["title"],
            "--description", f"Living dataset â€” auto-refreshed v{target['version']+1}. Topic: {topic_text}",
            "--files", file_path,
            "--visibility", topic.get("visibility", "public"),
        ]
        if demo:
            pub_args.append("--demo")
        elif api_key:
            pub_args.extend(["--api-key", api_key])

        pub_result = subprocess.run(pub_args, capture_output=True, text=True)
        print(pub_result.stdout)
    else:
        print("  â„¹ï¸  Skipping publish (no file)")

    # â”€â”€ Update registry â”€â”€
    for d in registry["living_datasets"]:
        if d["id"] == ld_id:
            d["version"]       += 1
            d["last_refreshed"]  = now.isoformat() + "Z"
            d["next_refresh"]    = next_run
            d["refresh_count"]  += 1
            d["freshness_score"] = 1.0
            d["status"]          = "active"
            d["changelog"].append({
                "version": d["version"],
                "date":    now.isoformat() + "Z",
                "summary": new_findings.get("changelog_summary", "Refreshed")
            })
            break

    save_json(REGISTRY_FILE, registry)

    # â”€â”€ Vault Integration: contribute refreshed dataset summary â”€â”€
    _auto = os.environ.get("VAULT_AUTO_APPROVE", "false").lower() == "true"
    _vault_content = (
        f"# {target['title']} â€” Refresh v{target['version']+1}\n"
        f"Topic: {topic_text}\n"
        f"Refreshed: {now.isoformat()}Z\n"
        f"Schedule: {schedule}\n"
        f"Changes: {new_findings.get('changelog_summary', 'Refreshed')}\n"
        f"Next refresh: {next_run[:16]}\n"
        f"Status: active\n"
    )
    vault_contribute(_vault_content, target["title"], auto_approve=_auto)

    print(f"\n{'â•'*60}")
    print(f"  âœ…  Refresh complete!")
    print(f"  Dataset: {target['title']}")
    print(f"  Version: v{target['version']+1}")
    print(f"  Next refresh: {next_run[:16].replace('T',' ')} UTC")
    print(f"{'â•'*60}\n")


def main():
    parser = argparse.ArgumentParser(description="Inflectiv Living Dataset Refresh Engine")
    parser.add_argument("--id",      help="Living dataset ID to refresh")
    parser.add_argument("--all",     action="store_true", help="Refresh all due datasets")
    parser.add_argument("--demo",    action="store_true", help="Demo mode")
    parser.add_argument("--api-key", default=os.environ.get("INFLECTIV_API_KEY"))
    args = parser.parse_args()

    if args.all:
        registry = load_json(REGISTRY_FILE)
        now = datetime.utcnow()
        due = []
        for d in registry.get("living_datasets", []):
            try:
                next_run = datetime.fromisoformat(d["next_refresh"].replace('Z',''))
                if now >= next_run:
                    due.append(d["id"])
            except Exception:
                pass
        if not due:
            print("âœ… All datasets are fresh â€” nothing due for refresh.")
            return
        print(f"ðŸ”„ {len(due)} dataset(s) due for refresh...")
        for ld_id in due:
            refresh_dataset(ld_id, demo=args.demo, api_key=args.api_key)
    elif args.id:
        refresh_dataset(args.id, demo=args.demo, api_key=args.api_key)
    else:
        parser.print_help()


if __name__ == "__main__":
    main()
